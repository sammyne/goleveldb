# compaction

Compaction 是 leveldb 最为复杂的过程之一，同样也是 leveldb 的性能瓶颈之一。其本质是一种内部数据重合整合的机制，同样也是一种平衡读写速率的有效手段，因此在下文中，首先介绍下 leveldb 中设计 compaction 的原由，再来介绍下 compaction 的具体过程。

## Compaction 的作用

### 数据持久化

leveldb 是典型的 LSM 树实现，因此需要对内存中的数据进行持久化。一次内存数据的持久化过程，在 leveldb 中称为 **Minor Compaction**。

一次 minor compaction 的产出是一个 0 层的 sstable 文件，其中包含了所有的内存数据。但是若干个 0 层文件中是可能存在数据 overlap 的。

### 提高读写效率

正如前面的文章提到，leveldb 是一个写效率十分高的存储引擎，存储的过程非常简单，只需要一次顺序的文件写和一个时间复杂度为 `O(log n)` 的内存操作即可。

相比来说，leveldb 的读操作就复杂不少。首先一到两次读操作需要进行一个复杂度为 O(log n) 的查询操作。若没有在内存中命中数据，则需要在按照数据的新旧程度在 0 层文件中依次进行查找**遍历**。由于 0 层文件中可能存在 overlap，因此在最差情况下，可能需要遍历所有的文件。

假设 leveldb 中就是以这样的方式进行数据维护，那么随着运行时间的增长，0 层的文件个数会越来越多，在最差的情况下，查询一个数据需要遍历**所有的数据文件**，这显然是不可接受的。因此 leveldb 设计了一个 **Major Compaction** 的过程，将 0 层中的文件合并为若干个没有数据重叠的 1 层文件。

对于没有数据重叠的文件，一次查找过程就可以进行优化，最多只需要一个文件的遍历即可完成。因此，leveldb 设计 compaction 的目的之一就是为了**提高读取的效率**。

### 平衡读写差异

有了 minor compaction 和 major compaction，所有的数据在后台都会被规定的次序进行整合。但是一次 major
compaction 的过程其本质是一个**多路归并**的过程，既有大量的磁盘读开销，也有大量的磁盘写开销，显然这是一个严重的性能瓶颈。

但是当用户写入的速度始终大于 major compaction 的速度时，就会导致 0 层的文件数量还是不断上升，用户的读取效率持续下降。所以 leveldb 中规定：

- 当 0 层文件数量超过 `SlowdownTrigger` 时，写入的速度主要减慢；
- 当 0 层文件数量超过 `PauseTrigger` 时，写入暂停，直至 Major Compaction 完成；

故 compaction 也可以起到平衡读写差异的作用。

### 整理数据

leveldb 的每一条数据项都有一个版本信息，标识着这条数据的新旧程度。这也就意味着同样一个 key，在 leveldb 中可能存在着多条数据项，且每个数据项包含了不同版本的内容。

为了尽量减少数据集所占用的磁盘空间大小，leveldb 在 major compaction 的过程中，对不同版本的数据项进行合并。

## Compaction 过程

由上述所示，compaction 分为两类：

- minor compaction
- major compaction

这两类 compaction 负责在不同的场景下进行不同的数据整理。

### Minor Compaction

一次 minor compaction 非常简单，其本质就是将一个内存数据库中的所有数据持久化到一个磁盘文件中。

![minor compaction](./images/minor.jpeg)

每次 minor compaction 结束后，都会生成一个新的 sstable 文件，也意味着 **Leveldb 的版本状态发生了变化，会进行一个版本的更替**。有关版本控制的内容，将在接下去一篇文章中详细展开。

值得注意的是，minor compaction 是一个时效性要求非常高的过程，要求其在尽可能短的时间内完成，否则就会堵塞正常的写入操作，因此 **minor compaction 的优先级高于 major compaction**。当进行 minor compaction 的时候有 major compaction 正在进行，则会首先暂停 major compaction。

### Major Compaction

相比于 minor compaction，major compaction 就会复杂地多。首先看一下一次 major compaction 的示意图。

![major compaction](./images/major.jpeg)

0 层中浅蓝色的三个 sstable 文件，加上 1 层中的绿色的 sstable 文件，四个文件进行了合并，输出成两个按序组织的新的 1 层 sstable 文件进行替换。

#### 条件

那么什么时候，会触发 leveldb 进行 major compaction 呢。总结地来说为以下三个条件：

- 当 0 层文件数超过预定的上限（默认为 4 个）；
- 当 level i 层文件的总大小超过 `10^i` MB；
- 当某个文件无效读取的次数过多；

##### 0 层文件个数规定

由于 compaction 的其中一个目的是为了提高读取的效率，因此 leveldb 不允许 0 层存在过多的文件数，一旦超过了上限值，即可进行 major compaction。

##### 非 0 层文件数据大小限制

对于 level i（i>=1）的情况来说，一个读取最多只会访问一个 sstable 文件，因此，本身对于读取效率的影响不会太大。针对于这部分数据发生 compaction 的条件，从 **提升读取效率** 转变成了 **降低compaction的IO开销**。

假设 leveldb 的合并策略只有第一条，那么会导致 1 层文件的个数越来越多或者总的数据量越来越大，而通常一次合并中，**0 层文件 key 的取值范围是很大的**，**导致每一次 0 层文件与 1 层文件进行合并时，1 层文件输入文件的总数据量非常庞大**。所以不仅需要控制 0 层文件的个数，同样，每一层文件的总大小同样需要进行控制，使得每次进行 compaction 时，IO 开销尽量保持常量。

故 leveldb 规定，1 层文件总大小上限为 10MB，2 层为 100MB，依次类推，最高层（7层）没有限制。

##### 文件读取次数过多

以上两个机制能够保证随着合并的进行，数据是严格下沉的，但是仍然存在一个问题。

假设 0 层文件完成合并之后，1 层文件同时达到了数据上限，同时需要进行合并。更加糟糕的是，在最差的情况下，0-n 层的文件同时达到了合并的条件，每一层都需要进行合并。

其中一种优化机制是：
- `source` 层的文件个数只有一个；
- `source` 层文件与 `source+1` 层文件没有重叠；
- `source` 层文件与 `source+2` 层的文件重叠部分不超过 10 个文件；

当满足这几个条件时，可以将 `source` 层的该文件直接移至 `source+1` 层。

但是该条件非常苛刻，还是无法解决上述问题。为了避免可能存在这种“巨大”的合并开销，leveldb 引入了第三个机制：”错峰合并“。

那么
1. 如何找寻这种适合错峰合并的文件
2. 以及如果判断哪个时机是适合进行错峰合并的呢？

对于问题 1，Leveldb 的作者认为，一个文件一次查询的开销为 10ms，**若某个文件的查询次数过多，且查询在该文件中不命中**，那么这种行为就可以视为无效的查询开销，这种文件就可以进行错峰合并。

对于问题 2，对于一个 1MB 的文件，对其合并的开销为 25ms。因此当一个文件 1MB 的文件无效查询超过 25 次时，便可以对其进行合并。

对于一个 1MB 的文件，其合并开销为
1. `source` 层 1MB 的文件读取
2. `source+1` 层 10-12MB 的文件读取
3. `source+1` 层 10-12MB 的文件写入

总结 25MB 的文件 IO 开销，除以 100 MB/s 的文件 IO 速度，估计开销为 25ms。

#### 采样探测

在每个 sstable 文件的元数据中，还有一个额外的字段 `seekLeft`，默认为文件的大小除以 16KB。

leveldb 在正常的数据访问时，会顺带进行采样探测。正常的数据访问包括
1. 用户直接调用 `Get` 接口
2. 用户使用迭代器进行访问

采样的规则：

记录本次访问的第一个 sstable 文件。若在该文件中访问命中，则不做任何处理；若在该文件中访问不命中，则对
该文件的 `seekLeft` 标志做减一操作。

知道某一个文件的 `seekLeft` 标志减少到 0 时，触发对该文件的错峰合并。

故以上三种机制可以保障每次进行 compaction 的时候，总体开销不会呈现上升趋势。

### 过程

整个 compaction 可以简单地分为以下几步：

1. 寻找合适的输入文件；
2. 根据 key 重叠情况扩大输入文件集合；
3. 多路合并；
4. 积分计算；

#### 寻找输入文件

不同情况下发起的合并动作，其初始的输入文件不同。

对于 *level 0 层文件数过多引发的合并场景或由于 level i 层文件总量过大的合并场景*，采用轮转的方法选择起始输入文件，记录了上一次该层合并的文件的最大 key，下一次则选择在此 key 之后的首个文件。

对于*错峰合并*，起始输入文件则为该查询次数过多的文件。

#### 扩大输入文件集合

该过程如下：

1. 红星标注的为起始输入文件；
2. 在 level i 层中，查找与起始输入文件有 key 重叠的文件，如图中红线所标注，最终构成 level i 层的输入文件；
3. 利用 level i 层的输入文件，在 level i+1 层找寻有 key 重叠的文件，结果为绿线标注的文件，构成 level i，i+1 层的输入文件；
4. 最后利用两层的输入文件，在不扩大 level i+1 输入文件的前提下，查找 level i 层的有 key 重叠的文件，结果为蓝线标准的文件，构成最终的输入文件；

![扩大输入文件集合示意图](./images/expand.jpeg)

#### 多路合并

多路合并的过程比较简单，即将 level i 层的文件，与 level i+1 层的文件中的数据项，按序整理之后，输出到 level i+1 层的若干个新文件中，即合并完成。

![多路合并](./images/merge.jpeg)

注意在整理的过程中，需要将冗余的数据进行清理，即同一条数据的多个版本信息，只保留最新的那一份。

但是要注意，某些仍然在使用的旧版本的数据，在此时不能立刻删除，而得等到用户使用结束，释放句柄后，根据引用计数来进行清除。

#### 积分计算

每一次 compaction 都会消除若干 `source` 层的旧文件，新增 `source+1` 层的新文件，因此触发进行合并的条件状态可能也发生了变化。故在 leveldb 中，使用了计分牌来维护每一层文件的文件个数及数据总量信息，来 **挑选出下一个需要进行合并的层数**。

计分的规则很简单：

-  对于 0 层文件，该层的分数为文件总数／4；
-  对于非 0 层文件，该层的分数为文件数据总量／数据总量上限；

将得分最高的层数记录，若该得分超过 1，则为下一次进行合并的层数；

## 用户行为

由于 leveldb 内部进行 compaction 时有 trivial move 优化，且根据内部的文件格式组织，用户在使用 leveldb 时，可以尽量将大批量需要写入的数据进行预排序，利用空间局部性，尽量减少多路合并的 IO 开销。
